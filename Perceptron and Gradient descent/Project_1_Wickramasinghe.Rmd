---
title: "STAT 6390 - Deep Learning - Mini Project 1\\vspace{-0.3cm} "
author: "Nisansala Wickramasinghe \\vspace{-0.5cm} "

output:
  bookdown::pdf_document2:
    toc: false
    keep_tex: true
    number_sections: false
    extra_dependencies: ["flafter"]

header-includes: 
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{subfig}
  - \usepackage{amsmath}
  - \usepackage{graphicx}
  - \usepackage{multicol}
  - \newcommand{\btwocol}{\begin{multicols}{2}}
  - \newcommand{\etwocol}{\end{multicols}}
  
geometry: "left=1cm,right=1cm,top=1cm,bottom=1.5cm"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(xtable.comment = FALSE)
knitr::opts_chunk$set(dev = 'pdf')
```

```{r,echo=FALSE}
library(knitr)
opts_chunk$set(comment="",warning = FALSE, message=FALSE,tidy.opts=list(keep.blank.line=TRUE, width.cutoff=120),options(width=100), cache=TRUE,fig.align='center',fig.height=6, fig.width=10,fig.path='figure/beamer-',fig.show='hold',size='footnotesize', cache=TRUE)
```

```{r include=FALSE}
setwd("/Users/nissi_wicky/Documents/UTD/11) 2023 Spring/STAT 6390 Deep Learning/Deep Learning Mini Projects/Project 1")
```


*** 


### Question 1 

a) **Figure** \@ref(fig:q1d) shows plot of training data. Data seems linearly separable. If we take  45$^\circ$ line as the linear decision boundary we do not see overlapping red and blue points near the boundary. Decision boundary is plotted in **Figure** \@ref(fig:q1d)

```{r include=FALSE}
train.data = read.csv("1-training-data.csv", header = TRUE)
str(train.data)
train.X = subset(train.data, select = c("x1.train", "x2.train"))
train.Y = train.data$y.train
test.data = read.csv("1-test-data.csv", header = TRUE)
str(test.data)
test.X = subset(test.data, select = c("x1.test", "x2.test"))
test.Y = test.data$y.test
```


b) R code for the implementation of perceptron can be found in section 2. Percepron criterion was used as the loss function. For a misclassified point, the gradient of loss function is $-y\overrightarrow{x} = - \{(y - \hat{y})/2\}\overrightarrow{x}$. Then the gradient-descent update becomes $\overrightarrow{w} \leftarrow \overrightarrow{w} + \alpha(y-\hat{y})\overrightarrow{x}$. Algorithm converge at 4$^\text{th}$ epoch (see **Figure** \@ref(fig:q1d)). 

    Training error : 0
    
    Test error : 0.005

```{r include=FALSE}

######## 1b) Percepron criterion ###########

# define activation function
sign_func <- function(z){
  return(if (z < 0) -1 else 1)
}

# function to calculate derivative of perceptron loss
perceptron_loss_prime <- function(y, yhat, x){
  return(- (y - yhat) * x)
}

perceptron <- function(X, Y, l_rate, epochs, max_iter){
  # Insering 1 for bias, x0 = 1 to the dataframe.
  X <- data.frame(x0 = 1, X)

  # Initializing weights to zeros
  weights  <- rep(0, dim(X)[2])

  # Initializing errors vector
  errors <- rep(0, epochs)

  for(j in 1: epochs){
    for(i in 1:length(Y)){
      x <- as.numeric(X[i,])
      y <- Y[i]
  
      # Forward propagation and calculating prediction 
      yhat = sign_func(weights %*% x)
  
      #Calculate errors using perceptron criterion
      if (max(-y * yhat, 0) != 0.0) {
        errors[j] <- errors[j] + 1
      }
  
      #Updating weight
      deltaL = perceptron_loss_prime(y, yhat, x)
      weights <- weights - l_rate * deltaL
    }
    #stopping criterion
    if (epochs > max_iter){
      print("exceeds maximum number of iteration")
      break
    }
    #print(paste("Epoch", j,"/",epochs,"| loss", errors[j]/length(Y)))
  }
  return(list(weights = weights, errors = errors/length(Y)))
}

epochs = 20
l_rate = 1

out = perceptron(train.X,train.Y, l_rate, epochs, 100)
#print("Training data: \n")
#out

#plot(1:epochs, err, type="l", lwd=2, col="red", xlab="epochs", ylab="errors")
```


```{r include=FALSE}
# make prediction
y_all = as.matrix(data.frame(x0 = 1, test.X)) %*% as.matrix(out$weights)

y_pred = y_all
y_pred[y_all >= 0] = 1
y_pred[y_all< 0] = -1

err = sum(y_pred != test.Y)/length(test.Y)
#print("Test data: \n")
#print(err)
```

c) R code for the implementation of perceptron can be found in section 2. Hinge loss was used as the loss function. For a misclassified point, the gradient of hinge loss is $-y\overrightarrow{x} = - \{(y - \hat{y})/2\}\overrightarrow{x}$ which is the same as the grdient of perceptron criterion. Then the gradient-descent update becomes $\overrightarrow{w} \leftarrow \overrightarrow{w} + \alpha(y-\hat{y})\overrightarrow{x}$. Therefore results are similar to part b) and algorithm converge at 4$^\text{th}$ epoch (see **Figure** \@ref(fig:q1d)). 

    Training error : 0
    
    Test error : 0.005
    
```{r include=FALSE}

######## 1c) Using Hinge loss ###########

# define activation function
sign_func <- function(z){
  return(if (z < 0) -1 else 1)
}

# function to calculate derivative of perceptron loss
hinge_loss_prime <- function(y, yhat, x){
  return(- (y - yhat) * x)
}

perceptron <- function(X, Y, l_rate, epochs, max_iter){
  # Insering 1 for bias, x0 = 1 to the dataframe.
  X <- data.frame(x0 = 1, X)

  # Initializing weights to zeros
  weights  <- rep(0, dim(X)[2])

  # Initializing errors vector
  errors <- rep(0, epochs)

  for(j in 1: epochs){
    for(i in 1:length(Y)){
      x <- as.numeric(X[i,])
      y <- Y[i]
  
      # Forward propagation and calculating prediction 
      yhat = sign_func(weights %*% x)
  
      #Calculate errors using hinge loss
      if (max(1 - y * yhat, 0) != 0.0) {
        errors[j] <- errors[j] + 1
      }
  
      #Updating weight
      deltaL = hinge_loss_prime(y, yhat, x)
      weights <- weights - l_rate * deltaL
    }
    #stopping criterion
    if (epochs > max_iter){
      print("exceeds maximum number of iteration")
      break
    }
    #print(paste("Epoch", j,"/",epochs,"| loss", errors[j]/length(Y)))
  }
  return(list(weights = weights, errors = errors/length(Y)))
}

epochs = 20
l_rate = 1

out_hinge = perceptron(train.X,train.Y, l_rate, epochs, 100)
#print("Training data: \n")
#out_hinge
```


```{r include=FALSE}
# make prediction
y_all = as.matrix(data.frame(x0 = 1, test.X)) %*% as.matrix(out_hinge$weights)

y_pred = y_all
y_pred[y_all >= 0] = 1
y_pred[y_all< 0] = -1

err = sum(y_pred != test.Y)/length(test.Y)
#print("Test data: \n")
#print(err)
```

```{r q1bc, eval=FALSE, fig.height=3, fig.width=6, include=FALSE}
par(mfrow=c(1,2))
plot(1:epochs, out$errors, type="l", lwd=2, col="red", xlab="epochs", ylab="errors")
plot(1:epochs, out_hinge$errors, type="l", lwd=2, col="red", xlab="epochs", ylab="errors")
```


d) Decision boundaries of classifiers obtained in parts b) and c) are plotted in **Figure** \@ref(fig:q1d). Decision boundries for part b) and c) are quite similar with my guess.

```{r q1d, echo=FALSE, fig.cap="\\textit{Left: Decision boundaries for part a),b),c) Right: training error vs epochs}",fig.height=4, fig.width=4, fig.show='hold'}

# Save current graphical parameters
opar <- par(no.readonly = TRUE)

# Change the margins of the plot 
# (the fourth is the right margin)
#Drawing Decision boundary
plot(train.X, pch = ifelse(train.Y == 1, 19, 4),
     col = ifelse(train.Y == 1 , "blue", "red"), cex = 0.7, cex.lab=0.7, cex.axis=0.7)
abline(0,1, col="black", lwd=1, lty=1)
abline(a = -1.0*out$weights[1]/out$weights[3], b = -1.0*out$weights[2]/out$weights[3], col="green", lwd=1, lty=2)
abline(a = -1.0*out_hinge$weights[1]/out_hinge$weights[3], b = -1.0*out_hinge$weights[2]/out_hinge$weights[3], col="purple", lwd=1, lty=3)
legend(x = "bottomright", 
       legend = c("a", "b", "c"), 
       lty = c(1, 2, 3),
       col = c("black","green","purple"),
       lwd = 1,
       xpd = TRUE) # Needed to put the legend outside the plot

# Back to the default graphical parameters

plot(1:epochs, out$errors, type="l", lwd=2, col="red", xlab="epochs", ylab="errors",cex = 0.7, cex.lab=0.7, cex.axis=0.7)
```

e) Both classifiers, classifier with perceptron criterion and classifier with hinge loss, gives the same test error for the data provided as they have the same gradient and weight update rule is the same. Data are linearly separable. Therefore both classifiers performs well with 0.005 test error rate.

## Question 2

a) The scatterplots and correlation matrix (see **Figure** \@ref(fig:q2a)) show that there is a positive correlation between response variable `Sales` and predictor variables `TV` and `radio`. The points in Q-Q plot deviate from the straight line suggesting that normality assumption is violated.

```{r include=FALSE}
Advertising <- read.csv("Advertising.csv", header = TRUE)
Advertising <- Advertising[,-1]
full.model <- lm(sales~ TV + radio + newspaper , data = Advertising)
```



```{r q2a, echo=FALSE, fig.cap="\\textit{Exploratory analysis. Left:Scatterplot matrix for Advertising data. Middle:Q-Q plot, Right:Residual vs fitted value}",fig.height=3, fig.width=3, fig.show='hold'}
chart.Correlation(Advertising)

qqnorm(full.model$residuals, main = " ", xlab = "Expected value", ylab = "Residual",pch = 19,cex.lab=0.5,xaxt="n",yaxt="n",cex.main=0.5)
qqline(full.model$residuals)
axis(2,cex.axis=0.5)
axis(1,cex.axis=0.5)

plot(x = full.model$fitted.values,  main = " ", y = full.model$residuals, abline(0,0), xlab = "Fitted Value",ylab = "Residual",cex.lab=0.5,xaxt="n",yaxt="n",cex.main=0.5)
axis(2,cex.axis=0.5)
axis(1,cex.axis=0.5)
```

b) Summary statistics for the model with all variables given below. We **normalized** $\mathbf{X}$ as they are in different scales. Based on the summary, we reject the null hypothesis $H0 : \beta_j = 0$ for the predictor `TV` and `radio``. The results are consistent with what we found in the exploratory analysis in part (a).

    The least squares method is used for developing estimates. Consider linear regression model $\mathbf{Y} = \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}$. Then fitted values will be $\mathbf{\hat{Y}}=\mathbf{X}\mathbf{\beta}$. We find least square estimates $\mathbf{\hat{\beta}}$ by minimizing the sum of squares of residuals. $\mathbf{\epsilon^{\prime}\epsilon} = \mathbf{(Y-X\beta)^{\prime}(Y-X\beta)}$. To minimize $\mathbf{\epsilon^{\prime}\epsilon}$ w.r.t $\mathbf{\beta}$, solve the normal equations $\partial\mathbf{\epsilon^{\prime}\epsilon}/\partial\mathbf{\beta} = -2\mathbf{X^{\prime} Y} +2\mathbf{X^{\prime}X\beta}= 0$ and we get least square estimates $\mathbf{\hat{\beta} = (X^{\prime}X)^{-1}X^{\prime}Y}$.
    
```{r include=FALSE}
X = Advertising[,1:3]
y = Advertising[,4]

X = apply(X, 2, function(x) (x-mean(x))/sd(x))
norm_Advertising <- as.data.frame(cbind(X,y))
```

```{r include=FALSE}
library(PerformanceAnalytics)
full.model <- lm(y ~ TV + radio + newspaper , data = norm_Advertising)
```

```{r echo=FALSE}
summary(full.model)
```

c) R code for the implementation of gradient-descent algorithm can be found in section 2. Algorithm converge when number of iterations is 1000 and learning rate 0.001. The estimates obtained in part(b) are similar to the estimates obtained in part(c).

```{r echo=FALSE}
Gradient_descent <- function(X, y, l_rate, n_iter, max_iter){
  X <- as.matrix(data.frame(Intercept = 1, X))
  
  weights  <- matrix(rep(0, dim(X)[2]), ncol=1)
  
  MSE <- rep(0, n_iter)
  
  for (i in 1:n_iter) {
    yhat <- X %*% weights
    MSE[i] <- sum( (y - yhat)^2 ) / length(y)
    deltaL = -t(y-yhat) %*% X
    weights <- weights - l_rate * t(deltaL)
    if (n_iter > max_iter){
      print("exceeds maximum number of iteration")
      break
    }
  }
  return(list(weights = weights, errors = MSE[n_iter]))
}

out = Gradient_descent(X,y, 0.001, 1000,10000)
print("coefficients")
out$weights
```

\newpage

\centering 

## Section 2: R code

```{r echo = T, results = 'hide',error=FALSE,warning=FALSE,message=FALSE,eval=FALSE}
## ----include=FALSE---------------------------------------------------------------
#Data preprocessing 
train.data = read.csv("1-training-data.csv", header = TRUE)
str(train.data)
train.X = subset(train.data, select = c("x1.train", "x2.train"))
train.Y = train.data$y.train
test.data = read.csv("1-test-data.csv", header = TRUE)
str(test.data)
test.X = subset(test.data, select = c("x1.test", "x2.test"))
test.Y = test.data$y.test


## ----include=FALSE---------------------------------------------------------------

######## 1a) ###########
# Can be found in 1d)

######## 1b) Percepron criterion ###########

# define activation function
sign_func <- function(z){
  return(if (z < 0) -1 else 1)
}

# function to calculate derivative of perceptron loss
perceptron_loss_prime <- function(y, yhat, x){
  return(- (y - yhat) * x)
}

perceptron <- function(X, Y, l_rate, epochs, max_iter){
  # Insering 1 for bias, x0 = 1 to the dataframe.
  X <- data.frame(x0 = 1, X)

  # Initializing weights to zeros
  weights  <- rep(0, dim(X)[2])

  # Initializing errors vector
  errors <- rep(0, epochs)

  for(j in 1: epochs){
    for(i in 1:length(Y)){
      x <- as.numeric(X[i,])
      y <- Y[i]
  
      # Forward propagation and calculating prediction 
      yhat = sign_func(weights %*% x)
  
      #Calculate errors using perceptron criterion
      if (max(-y * yhat, 0) != 0.0) {
        errors[j] <- errors[j] + 1
      }
  
      #Updating weight
      deltaL = perceptron_loss_prime(y, yhat, x)
      weights <- weights - l_rate * deltaL
    }
    #stopping criterion
    if (epochs > max_iter){
      print("exceeds maximum number of iteration")
      break
    }
    #print(paste("Epoch", j,"/",epochs,"| loss", errors[j]/length(Y)))
  }
  return(list(weights = weights, errors = errors/length(Y)))
}

epochs = 20
l_rate = 1

out = perceptron(train.X,train.Y, l_rate, epochs, 100)
#print("Training data: \n")
#out

#plot(1:epochs, err, type="l", lwd=2, col="red", xlab="epochs", ylab="errors")


## ----include=FALSE---------------------------------------------------------------
# make prediction
y_all = as.matrix(data.frame(x0 = 1, test.X)) %*% as.matrix(out$weights)

y_pred = y_all
y_pred[y_all >= 0] = 1
y_pred[y_all< 0] = -1

err = sum(y_pred != test.Y)/length(test.Y)
#print("Test data: \n")
#print(err)


## ----include=FALSE---------------------------------------------------------------

######## 1c) Using Hinge loss ###########

# define activation function
sign_func <- function(z){
  return(if (z < 0) -1 else 1)
}

# function to calculate derivative of perceptron loss
hinge_loss_prime <- function(y, yhat, x){
  return(- (y - yhat) * x)
}

perceptron <- function(X, Y, l_rate, epochs, max_iter){
  # Insering 1 for bias, x0 = 1 to the dataframe.
  X <- data.frame(x0 = 1, X)

  # Initializing weights to zeros
  weights  <- rep(0, dim(X)[2])

  # Initializing errors vector
  errors <- rep(0, epochs)

  for(j in 1: epochs){
    for(i in 1:length(Y)){
      x <- as.numeric(X[i,])
      y <- Y[i]
  
      # Forward propagation and calculating prediction 
      yhat = sign_func(weights %*% x)
  
      #Calculate errors using hinge loss
      if (max(1 - y * yhat, 0) != 0.0) {
        errors[j] <- errors[j] + 1
      }
  
      #Updating weight
      deltaL = hinge_loss_prime(y, yhat, x)
      weights <- weights - l_rate * deltaL
    }
    #stopping criterion
    if (epochs > max_iter){
      print("exceeds maximum number of iteration")
      break
    }
    #print(paste("Epoch", j,"/",epochs,"| loss", errors[j]/length(Y)))
  }
  return(list(weights = weights, errors = errors/length(Y)))
}

epochs = 20
l_rate = 1

out_hinge = perceptron(train.X,train.Y, l_rate, epochs, 100)
#print("Training data: \n")
#out_hinge


## ----include=FALSE---------------------------------------------------------------
# make prediction
y_all = as.matrix(data.frame(x0 = 1, test.X)) %*% as.matrix(out_hinge$weights)

y_pred = y_all
y_pred[y_all >= 0] = 1
y_pred[y_all< 0] = -1

err = sum(y_pred != test.Y)/length(test.Y)
#print("Test data: \n")
#print(err)


## ----q1bc, eval=FALSE, fig.height=3, fig.width=6, include=FALSE------------------
## par(mfrow=c(1,2))
## plot(1:epochs, out$errors, type="l", lwd=2, col="red", xlab="epochs", ylab="errors")
## plot(1:epochs, out_hinge$errors, type="l", lwd=2, col="red", xlab="epochs", ylab="errors")

######## 1d) Plotting Decision boundaries ###########

## ----q1d, echo=FALSE, fig.cap="\\textit{Plot for training data}",fig.height=4, fig.width=5----

# Save current graphical parameters
opar <- par(no.readonly = TRUE)

# Change the margins of the plot 
# (the fourth is the right margin)
par(mar = c(6, 5, 4, 6))

#Drawing Decision boundary
plot(train.X, pch = ifelse(train.Y == 1, 19, 4),
     col = ifelse(train.Y == 1 , "blue", "red"), cex = 0.7, cex.lab=0.7, cex.axis=0.7)
abline(0,1, col="black", lwd=1, lty=1)
abline(a = -1.0*out$weights[1]/out$weights[3], b = -1.0*out$weights[2]/out$weights[3], col="green", lwd=1, lty=2)
abline(a = -1.0*out_hinge$weights[1]/out_hinge$weights[3], b = -1.0*out_hinge$weights[2]/out_hinge$weights[3], col="purple", lwd=1, lty=3)
legend(x = "topright",
       inset = c(-0.35, 0), 
       legend = c("a", "b", "c"), 
       lty = c(1, 2, 3),
       col = c("black","green","purple"),
       lwd = 2,
       xpd = TRUE) # Needed to put the legend outside the plot

# Back to the default graphical parameters
on.exit(par(opar))

########################### Question 2 ############################################

######## 2a) exploratory data analysis ###########

## ----include=FALSE---------------------------------------------------------------
Advertising <- read.csv("Advertising.csv", header = TRUE)
Advertising <- Advertising[,-1]
full.model <- lm(sales~ TV + radio + newspaper , data = Advertising)

## ----q2a, echo=FALSE, fig.cap="\\textit{Exploratory analysis. Left:Scatterplot matrix for Advertising data. Middle:Q-Q plot, Right:Residual vs fitted value}",fig.height=3, fig.width=3, fig.show='hold'----
chart.Correlation(Advertising)

qqnorm(full.model$residuals, main = " ", xlab = "Expected value", ylab = "Residual",pch = 19,cex.lab=0.5,xaxt="n",yaxt="n",cex.main=0.5)
qqline(full.model$residuals)
axis(2,cex.axis=0.5)
axis(1,cex.axis=0.5)

plot(x = full.model$fitted.values,  main = " ", y = full.model$residuals, abline(0,0), xlab = "Fitted Value",ylab = "Residual",cex.lab=0.5,xaxt="n",yaxt="n",cex.main=0.5)
axis(2,cex.axis=0.5)
axis(1,cex.axis=0.5)

######## 2b) Fit linear regression model ###########

## ----include=FALSE---------------------------------------------------------------
X = Advertising[,1:3]
y = Advertising[,4]

#normalize data
X = apply(X, 2, function(x) (x-mean(x))/sd(x))
norm_Advertising <- as.data.frame(cbind(X,y))

## ----include=FALSE---------------------------------------------------------------
library(PerformanceAnalytics)
full.model <- lm(y ~ TV + radio + newspaper , data = norm_Advertising)


## ----echo=FALSE------------------------------------------------------------------
summary(full.model)

######## 2c) Gradient descent algorithm ###########

## ----echo=FALSE------------------------------------------------------------------
Gradient_descent <- function(X, y, l_rate, n_iter, max_iter){
  X <- as.matrix(data.frame(Intercept = 1, X))
  
  weights  <- matrix(rep(0, dim(X)[2]), ncol=1)
  
  MSE <- rep(0, n_iter)
  
  for (i in 1:n_iter) {
    yhat <- X %*% weights
    MSE[i] <- sum( (y - yhat)^2 ) / length(y)
    deltaL = -t(y-yhat) %*% X
    weights <- weights - l_rate * t(deltaL)
    if (n_iter > max_iter){
      print("exceeds maximum number of iteration")
      break
    }
  }
  return(list(weights = weights, errors = MSE[n_iter]))
}

out = Gradient_descent(X,y, 0.001, 1000,10000)
print("coefficients")
out$weights
```






